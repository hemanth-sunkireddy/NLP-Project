{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ca99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada7911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Loading Sentence-BERT model...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(\"Loaded Model...\")\n",
    "\n",
    "print(\"Loading Generated Questions...\")\n",
    "generated_qs_path = \"data/intermediate/generated_questions.txt\"\n",
    "if os.path.exists(generated_qs_path):\n",
    "    with open(generated_qs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        generated_questions = [line.strip() for line in f if line.strip()]\n",
    "    generated_embeddings = model.encode(generated_questions, convert_to_tensor=True)\n",
    "    print(\"Loaded Generated Questions.\")\n",
    "else:\n",
    "    print(\"'generated_questions.txt' not found!\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(\"Loading Courses and Institutions Data\")\n",
    "with open(\"data/processed/data.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "\n",
    "df_unique = df.drop_duplicates(subset=[\"reviews\"]).reset_index(drop=True)\n",
    "print(\"Loaded courses and institutions data.\")\n",
    "\n",
    "print(\"Loading Intent Classifier Model..\")\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "print(\"Loaded Intent Classifier Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad57c54",
   "metadata": {},
   "source": [
    "## Ask User to select a institution and Course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc850476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ask user to select an institution ---\n",
    "institutions = sorted(df_unique[\"institution\"].dropna().unique())\n",
    "print(\"\\nðŸ« Available Institutions:\")\n",
    "for idx, inst in enumerate(institutions):\n",
    "    print(f\"{idx + 1}. {inst}\")\n",
    "\n",
    "inst_idx = int(input(\"\\nðŸ‘‰ Select an institution by number: \")) - 1\n",
    "selected_institution = institutions[inst_idx]\n",
    "print(f\"\\nâœ… You selected: {selected_institution}\")\n",
    "\n",
    "# --- Filter courses ---\n",
    "inst_courses = sorted(df_unique[df_unique[\"institution\"] == selected_institution][\"name\"].dropna().unique())\n",
    "print(\"\\nðŸ“š Courses in this institution:\")\n",
    "for idx, course in enumerate(inst_courses):\n",
    "    print(f\"{idx + 1}. {course}\")\n",
    "\n",
    "course_idx = int(input(\"\\nðŸ‘‰ Select a course by number: \")) - 1\n",
    "selected_course = inst_courses[course_idx]\n",
    "print(f\"\\nâœ… You selected course: {selected_course}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063969bb",
   "metadata": {},
   "source": [
    "## Evaluationg Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_review_questions = [\n",
    "    \"How was the instructorâ€™s teaching?\",\n",
    "    \"Was the course content clear?\",\n",
    "    \"How effective were the assignments and projects?\",\n",
    "    \"Would you recommend it?\",\n",
    "     \"Were videos easy to follow?\",\n",
    "    \"How well did the instructor explain complex topics?\",\n",
    "    \"Were the assignments helpful for practice?\",\n",
    "    \"Was the course organized and easy to navigate?\",\n",
    "    \"Did you find the learning platform user-friendly?\",\n",
    "    \"What improvements would you suggest for this course?\"\n",
    "]\n",
    "\n",
    "non_course_review_questions = [\n",
    "    \"Whatâ€™s your hobby?\",\n",
    "    \"Do you like music?\",\n",
    "    \"How are you?\",\n",
    "    \"Beach or mountains?\",\n",
    "    \"What do you enjoy doing in your free time?\",\n",
    "    \"Have you traveled anywhere interesting recently?\",\n",
    "    \"Whatâ€™s your favorite way to relax after studying or working?\",\n",
    "    \"Is there a skill you'd love to master one day?\",\n",
    "    \"Do you prefer reading books or watching shows?\",\n",
    "    \"Favorite movie?\"\n",
    "]\n",
    "\n",
    "# Combine questions and assign labels\n",
    "all_questions = course_review_questions + non_course_review_questions\n",
    "true_labels = [1]*len(course_review_questions) + [0]*len(non_course_review_questions)\n",
    "\n",
    "# Generate embeddings for reference course-related questions\n",
    "reference_embeddings = generated_embeddings.cpu()\n",
    "\n",
    "# Store predictions and scores\n",
    "predicted_labels = []\n",
    "cosine_scores_list = []\n",
    "\n",
    "print(\"\\n--- Evaluation Results ---\\n\")\n",
    "\n",
    "# Calculate cosine similarity for each question\n",
    "for i, question in enumerate(all_questions):\n",
    "    query_embedding = model.encode(question, convert_to_tensor=True).cpu()\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, reference_embeddings)\n",
    "    max_score = torch.max(cosine_scores).item()\n",
    "    cosine_scores_list.append(max_score)\n",
    "\n",
    "# Evaluate different thresholds and calculate accuracy\n",
    "thresholds = np.arange(0.5, 1.1, 0.1)\n",
    "accuracy_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Predict labels based on cosine similarity threshold\n",
    "    predictions = (np.array(cosine_scores_list) >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predictions == true_labels).mean()\n",
    "    accuracy_results.append((threshold, accuracy))\n",
    "\n",
    "# Create a DataFrame for displaying results\n",
    "accuracy_df = pd.DataFrame(accuracy_results, columns=[\"Threshold\", \"Accuracy\"])\n",
    "\n",
    "# Find the best threshold and its accuracy\n",
    "best_threshold = accuracy_df.loc[accuracy_df[\"Accuracy\"].idxmax()]\n",
    "\n",
    "# Display results in a table format\n",
    "print(\"\\nThresholds and their corresponding accuracies:\")\n",
    "print(accuracy_df)\n",
    "\n",
    "print(f\"\\nBest Threshold: {best_threshold['Threshold']:.2f} with Accuracy: {best_threshold['Accuracy']*100:.2f}%\")\n",
    "\n",
    "print(\"Course Related Questions:\")\n",
    "for question in course_review_questions[-5:]:\n",
    "    print(f\" - {question}\")\n",
    "print()\n",
    "print(\" - ...\")\n",
    "\n",
    "print(\"Non Course Related Questions:\")\n",
    "for question in non_course_review_questions[-5:]:\n",
    "    print(f\" - {question}\")\n",
    "print(\" - ...\")\n",
    "\n",
    "\n",
    "# Display the last 3 questions from each category and cosine similarity\n",
    "print(\"Output of model from Course Related questions:\")\n",
    "for question in course_review_questions[:3]:\n",
    "    query_embedding = model.encode(question, convert_to_tensor=True).cpu()\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, reference_embeddings)\n",
    "    max_score = torch.max(cosine_scores).item()\n",
    "    predicted_label = 1 if max_score >= best_threshold['Threshold'] else 0\n",
    "    related = \"Related\" if predicted_label == 1 else \"Not Related\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Cosine Similarity: {max_score:.4f}\")\n",
    "    print(f\"Prediction: {related}\")\n",
    "    print()\n",
    "\n",
    "print(\"Output of model from Non-Course Related questions:\")\n",
    "for question in non_course_review_questions[:3]:\n",
    "    query_embedding = model.encode(question, convert_to_tensor=True).cpu()\n",
    "    cosine_scores = util.pytorch_cos_sim(query_embedding, reference_embeddings)\n",
    "    max_score = torch.max(cosine_scores).item()\n",
    "    predicted_label = 1 if max_score >= best_threshold['Threshold'] else 0\n",
    "    related = \"Related\" if predicted_label == 1 else \"Not Related\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Cosine Similarity: {max_score:.4f}\")\n",
    "    print(f\"Prediction: {related}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d4ce4-33e8-458c-8e2b-687c40f50f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intent Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_actions = {\n",
    "    \"yes_no\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"instructor\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"content\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"difficulty\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"career\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"general_opinion\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"course_overview\": {\n",
    "        \"sentiment\": False,\n",
    "        \"nlg\": False,\n",
    "        \"summarization\": True\n",
    "    },\n",
    "    \"prerequisites\": {\n",
    "        \"sentiment\": False,\n",
    "        \"nlg\": False,\n",
    "        \"summarization\": True\n",
    "    },\n",
    "    \"schedule\": {\n",
    "        \"sentiment\": False,\n",
    "        \"nlg\": False,\n",
    "        \"summarization\": True\n",
    "    },\n",
    "    \"fees\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    },\n",
    "    \"certification\": {\n",
    "        \"sentiment\": True,\n",
    "        \"nlg\": True,\n",
    "        \"summarization\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"{'Intent Type':<20} {'Sentiment':<10} {'NLG':<10} {'Summarization':<15}\")\n",
    "print(\"-\" * 55)\n",
    "for intent, actions in intent_actions.items():\n",
    "    print(f\"{intent:<20} {str(actions['sentiment']):<10} {str(actions['nlg']):<10} {str(actions['summarization']):<15}\")\n",
    "\n",
    "\n",
    "intent_labels_readable = [\n",
    "    \"Is the user asking a yes or no question?\",\n",
    "    \"Is the user asking about the instructor?\",\n",
    "    \"Is the user asking about the course content or topics?\",\n",
    "    \"Is the user asking about how difficult the course is?\",\n",
    "    \"Is the user asking about career outcomes or job relevance?\",\n",
    "    \"Is the user asking for general opinions from students?\",\n",
    "    \"Is the user asking for a summary or overview of the course?\",\n",
    "    \"Is the user asking about course prerequisites?\",\n",
    "    \"Is the user asking about the course schedule or duration?\",\n",
    "    \"Is the user asking about course fees or costs?\",\n",
    "    \"Is the user asking about course certification or accreditation?\"\n",
    "]\n",
    "\n",
    "label_map = {\n",
    "    \"Is the user asking a yes or no question?\": \"yes_no\",\n",
    "    \"Is the user asking about the instructor?\": \"instructor\",\n",
    "    \"Is the user asking about the course content or topics?\": \"content\",\n",
    "    \"Is the user asking about how difficult the course is?\": \"difficulty\",\n",
    "    \"Is the user asking about career outcomes or job relevance?\": \"career\",\n",
    "    \"Is the user asking for general opinions from students?\": \"general_opinion\",\n",
    "    \"Is the user asking for a summary or overview of the course?\": \"course_overview\",\n",
    "    \"Is the user asking about course prerequisites?\": \"prerequisites\",\n",
    "    \"Is the user asking about the course schedule or duration?\": \"schedule\",\n",
    "    \"Is the user asking about course fees or costs?\": \"fees\",\n",
    "    \"Is the user asking about course certification or accreditation?\": \"certification\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d056707-7e8a-4ed6-9d58-1af647bf5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"Enter your question related to the course: \")\n",
    "\n",
    "# Encode the query\n",
    "query_embedding = model.encode(query, convert_to_tensor=True).cpu()\n",
    "\n",
    "# Compute cosine similarity with reference (course-related) embeddings\n",
    "reference_embeddings = generated_embeddings.cpu()\n",
    "cosine_scores = util.pytorch_cos_sim(query_embedding, reference_embeddings)\n",
    "max_score = torch.max(cosine_scores).item()\n",
    "\n",
    "# Set threshold\n",
    "threshold = 0.7\n",
    "print(\"Cosine distance of the question: \", max_score)\n",
    "is_related = max_score >= threshold\n",
    "\n",
    "if is_related:\n",
    "    print(\"Related to the course reviews. Proceeding...\")\n",
    "    print(\"Question:\", query)\n",
    "    \n",
    "    result = classifier(query, candidate_labels=intent_labels_readable, multi_label=False)\n",
    "    # Print scores for each label\n",
    "    print(\"Intent Scores:\")\n",
    "    for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "        print(f\"{label:<60} {score:.4f}\")\n",
    "\n",
    "    # Get top intent and its score\n",
    "    predicted_label = result[\"labels\"][0]\n",
    "    predicted_score = result[\"scores\"][0]\n",
    "    predicted_intent = label_map[predicted_label]\n",
    "    actions = intent_actions[predicted_intent]\n",
    "\n",
    "    print(f\"\\nBest Intent: {predicted_intent} (Score: {predicted_score:.4f})\")\n",
    "\n",
    "    if actions[\"sentiment\"]:\n",
    "        print(\"Perform sentiment analysis\")\n",
    "\n",
    "    if actions[\"nlg\"]:\n",
    "        print(\"Generate answer using NLG\")\n",
    "    else:\n",
    "        print(\"Summarize relevant content instead of generating an answer\")\n",
    "\n",
    "else:\n",
    "    print(f\"Question is not related to the course (Similarity Score: {max_score:.2f}). Please enter a course-related question.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398c376-7169-4141-a2a2-b4dfecf14200",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Is this course suitable for beginners?\",\n",
    "    \"How experienced is the instructor?\",\n",
    "    \"What topics are covered in this course?\",\n",
    "    \"Is this course difficult?\",\n",
    "    \"Will this course help me in my career?\",\n",
    "    \"What do students think about this course?\",\n",
    "    \"Can you give me an overview of the course?\",\n",
    "    \"What are the prerequisites for this course?\",\n",
    "    \"What is the course schedule?\",\n",
    "    \"How much does the course cost?\",\n",
    "    \"Will I receive a certificate after completing the course?\"\n",
    "]\n",
    "\n",
    "true_intents = [\n",
    "    \"yes_no\",\n",
    "    \"instructor\",\n",
    "    \"content\",\n",
    "    \"difficulty\",\n",
    "    \"career\",\n",
    "    \"general_opinion\",\n",
    "    \"course_overview\",\n",
    "    \"prerequisites\",\n",
    "    \"schedule\",\n",
    "    \"fees\",\n",
    "    \"certification\"\n",
    "]\n",
    "\n",
    "correct_predictions = 0\n",
    "total_questions = len(questions)\n",
    "mispredicted_questions = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    result = classifier(question, candidate_labels=intent_labels_readable, multi_label=False)\n",
    "    predicted_label = result[\"labels\"][0]\n",
    "    predicted_intent = label_map[predicted_label]\n",
    "    print(f\"Predicted Intent: {predicted_intent}\")\n",
    "    if predicted_intent == true_intents[i]:\n",
    "        correct_predictions += 1\n",
    "    else:\n",
    "        mispredicted_questions.append((question, predicted_intent, true_intents[i]))\n",
    "\n",
    "accuracy = (correct_predictions / total_questions) * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}%\")\n",
    "\n",
    "if mispredicted_questions:\n",
    "    print(\"\\nMispredicted Questions:\")\n",
    "    for question, predicted, true in mispredicted_questions:\n",
    "        print(f\"Question: {question}\\nPredicted: {predicted}, Correct: {true}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36b329-f24b-4cf2-a073-450941711a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸ“„ Fetching reviews for the selected course...\")\n",
    "# Filter reviews for the selected course and institution\n",
    "filtered_reviews = df_unique[ (df_unique[\"institution\"] == selected_institution) & (df_unique[\"name\"] == selected_course) ][\"reviews\"].dropna().tolist()\n",
    "\n",
    "if not filtered_reviews: \n",
    "    print(\"No reviews found for the selected course.\") \n",
    "    exit()\n",
    "else:\n",
    "    print(\"Total Reviews for the course:\", len(filtered_reviews))\n",
    "\n",
    "# Encode reviews using Sentence-BERT\n",
    "print(\"Encoding reviews using Sentence-BERT...\") \n",
    "review_embeddings = model.encode(filtered_reviews, convert_to_tensor=True).cpu()\n",
    "print(\"Encoded all reviews.\")\n",
    "\n",
    "# Compute cosine similarity between query and each review\n",
    "cosine_scores = util.pytorch_cos_sim(query_embedding, review_embeddings)[0]\n",
    "\n",
    "# Filter reviews with similarity â‰¥ 0.7\n",
    "threshold = 0.5\n",
    "related_reviews = [\n",
    "    (filtered_reviews[i], score.item())\n",
    "    for i, score in enumerate(cosine_scores) if score >= threshold\n",
    "]\n",
    "\n",
    "print(\"Question: \", query)\n",
    "print(\"Related Sentences Count: \", len(related_reviews))\n",
    "if related_reviews:\n",
    "    print(f\"\\nReviews related to the query (threshold â‰¥ {threshold}): \\n\")\n",
    "    for review, score in sorted(related_reviews, key=lambda x: x[1], reverse=True):\n",
    "        print(f\"- Score: {score:.4f} | Review: {review}\")\n",
    "else:\n",
    "    print(f\"\\nNo reviews found with similarity â‰¥ {threshold}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf46244-c802-48a5-9679-55e281b53308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
